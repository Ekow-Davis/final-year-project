{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45425193-ce1a-45ba-9522-791683ef5780",
   "metadata": {},
   "source": [
    "MRI Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ff7ef-d09c-4433-b781-96e24c9b9124",
   "metadata": {},
   "source": [
    "We will perform teh following steps to the data\n",
    "Merge\n",
    "Resize\n",
    "Split and Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375fc1d-0b29-4cc4-b476-7b53c9ef7827",
   "metadata": {},
   "source": [
    "Merging both datasets of Training and Test from the orignial data and randomly splitting after resizeing to ensure the data is randomised and properly representing the system without deviating from the orginal implementation methods of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1c13f-6fcd-4d67-8b66-03186dc78251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "   ---------------------------------------- 0.0/39.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.4/39.0 MB 11.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.7/39.0 MB 11.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 6.8/39.0 MB 11.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.4/39.0 MB 11.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.5/39.0 MB 11.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 14.4/39.0 MB 11.8 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 16.8/39.0 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 19.4/39.0 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 22.3/39.0 MB 12.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.6/39.0 MB 12.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 27.3/39.0 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.9/39.0 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.5/39.0 MB 12.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.1/39.0 MB 12.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.7/39.0 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/39.0 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.0/39.0 MB 11.3 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.6/12.6 MB 12.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.2/12.6 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.6 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.5/12.6 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.6 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 10.5 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, opencv-python\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.2.6 opencv-python-4.12.0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ekowd\\anaconda3\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a383db-1516-47fd-a698-196b5954573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version: 4.12.0\n",
      "NumPy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5bf38c-2db0-47a8-87c5-ba7491f1e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba656e87-f677-4e33-9a62-dfd6ae151777",
   "metadata": {},
   "source": [
    "Since the raw data is already stored in the project repository, we'll call it from there rather than kaggle directly.\n",
    "\n",
    "We will define the specific path for the raw data folders and the file path for the folder we will store the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8622961f-c6cd-4386-837e-a269091cc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "RAW_TRAIN_DIR = Path(r\"data\\raw_data\\Training\")\n",
    "RAW_TEST_DIR  = Path(r\"data\\raw_data\\Testing\")\n",
    "CLEAN_DIR     = Path(r\"data\\cleaned_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7ff8ee-a0aa-4568-af60-83b62dfb9d51",
   "metadata": {},
   "source": [
    "Next we will create the folder structure in the clean directory since we plan to split the data into train, test and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ebece1e-9293-40e7-84bf-9d093ea7d3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder structure created under: data\\cleaned_data\n"
     ]
    }
   ],
   "source": [
    "# Configuration \n",
    "CLASSES = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "IMG_SIZE = (224, 224)\n",
    "SPLIT_RATIOS = {'train': 0.8, 'val': 0.1, 'test': 0.1}\n",
    "\n",
    "# Create Folder Structure \n",
    "for split in ['train', 'val', 'test']:\n",
    "    for cls in CLASSES:\n",
    "        path = CLEAN_DIR / split / cls\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Folder structure created under:\", CLEAN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c200f5c5-fef6-44d3-b9d5-3332541f7272",
   "metadata": {},
   "source": [
    "Next we prepare the function to Load, Resize and collect the images, resizing them to 224x224 which provide the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abd7ba1-f735-4861-bc5e-e21f3433e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Load, Resize, and Collect Image Paths\n",
    "def load_and_resize_images(class_name):\n",
    "    \"\"\"\n",
    "    Combines training and testing images for a given class,\n",
    "    resizes to 224x224, and returns a list of (image_array, filename).\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    # Combine both train and test directories\n",
    "    for data_dir in [RAW_TRAIN_DIR, RAW_TEST_DIR]:\n",
    "        class_dir = data_dir / class_name\n",
    "        if not class_dir.exists():\n",
    "            print(f\" Skipped missing directory: {class_dir}\")\n",
    "            continue\n",
    "        for img_file in class_dir.glob('*'):\n",
    "            try:\n",
    "                img = cv2.imread(str(img_file))\n",
    "                if img is None:\n",
    "                    continue\n",
    "                img_resized = cv2.resize(img, IMG_SIZE)\n",
    "                images.append((img_resized, img_file.name))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_file}: {e}\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203014da-ae32-4b07-ac95-b5f1e8f2df02",
   "metadata": {},
   "source": [
    "Then we store the now resized images, passing it to our saving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0b30100-3525-4e02-a0bf-5c78f3dd6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Save Images \n",
    "def save_images(images, save_dir, class_name):\n",
    "    \"\"\"\n",
    "    Saves a list of (image_array, filename) into a given directory.\n",
    "    \"\"\"\n",
    "    for img_array, filename in images:\n",
    "        save_path = save_dir / class_name / filename\n",
    "        cv2.imwrite(str(save_path), img_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6632b590-b8cf-4e36-96a9-63364f9edc20",
   "metadata": {},
   "source": [
    "Now we process the classes for each type of tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e871ca-4ae3-4230-b7d2-a511cb9ead79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing class: glioma\n",
      " Skipped missing directory: data\\raw_data\\Training\\glioma\n",
      " Skipped missing directory: data\\raw_data\\Testing\\glioma\n",
      "Total images found (merged): 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.19999999999999996 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m y = [\u001b[38;5;28mcls\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(images)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Split: train (80%) + temp (20%)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mSPLIT_RATIOS\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Split temp into val (10%) and test (10%)\u001b[39;00m\n\u001b[32m     16\u001b[39m X_val, X_test, y_val, y_test = train_test_split(\n\u001b[32m     17\u001b[39m     X_test, y_test, test_size=\u001b[32m0.5\u001b[39m, random_state=\u001b[32m42\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\David\\Desktop\\School\\DCIT400\\myenv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\David\\Desktop\\School\\DCIT400\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2919\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2916\u001b[39m arrays = indexable(*arrays)\n\u001b[32m   2918\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m n_train, n_test = \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\n\u001b[32m   2921\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   2924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\David\\Desktop\\School\\DCIT400\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2499\u001b[39m, in \u001b[36m_validate_shuffle_split\u001b[39m\u001b[34m(n_samples, test_size, train_size, default_test_size)\u001b[39m\n\u001b[32m   2496\u001b[39m n_train, n_test = \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[32m   2498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2499\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maforementioned parameters.\u001b[39m\u001b[33m\"\u001b[39m.format(n_samples, test_size, train_size)\n\u001b[32m   2503\u001b[39m     )\n\u001b[32m   2505\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[31mValueError\u001b[39m: With n_samples=0, test_size=0.19999999999999996 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Process Each Class\n",
    "for cls in CLASSES:\n",
    "    print(f\"\\n Processing class: {cls}\")\n",
    "    images = load_and_resize_images(cls)\n",
    "    print(f\"Total images found (merged): {len(images)}\")\n",
    "\n",
    "    # Prepare numpy arrays for splitting\n",
    "    X = np.arange(len(images))  # dummy indices for split\n",
    "    y = [cls] * len(images)\n",
    "\n",
    "    # Split: train (80%) + temp (20%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=(1 - SPLIT_RATIOS['train']), random_state=42, shuffle=True\n",
    "    )\n",
    "    # Split temp into val (10%) and test (10%)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_test, y_test, test_size=0.5, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Map indices to image data\n",
    "    train_imgs = [images[i] for i in X_train]\n",
    "    val_imgs   = [images[i] for i in X_val]\n",
    "    test_imgs  = [images[i] for i in X_test]\n",
    "\n",
    "    # Save into respective folders\n",
    "    print(\"   Saving training images...\")\n",
    "    save_images(tqdm(train_imgs, leave=False), CLEAN_DIR / 'train', cls)\n",
    "\n",
    "    print(\"   Saving validation images...\")\n",
    "    save_images(tqdm(val_imgs, leave=False), CLEAN_DIR / 'val', cls)\n",
    "\n",
    "    print(\"   Saving testing images...\")\n",
    "    save_images(tqdm(test_imgs, leave=False), CLEAN_DIR / 'test', cls)\n",
    "\n",
    "    print(f\" Done: {cls} | Train: {len(train_imgs)} | Val: {len(val_imgs)} | Test: {len(test_imgs)}\")\n",
    "\n",
    "print(\"\\n Dataset successfully cleaned, resized, and split!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec05d2-5fae-413e-b195-c4af5eeb6b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef27ef7-f77e-4d52-9d66-d90b2447639c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
